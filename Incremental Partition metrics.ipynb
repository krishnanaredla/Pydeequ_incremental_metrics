{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be315928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydeequ.analyzers import _AnalyzerObject\n",
    "from pydeequ.scala_utils import to_scala_seq\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "\n",
    "\n",
    "class IncreamentalAnalysis:\n",
    "    def __init__(self, analyzer: List[_AnalyzerObject] = []):\n",
    "        self.analyzer = analyzer\n",
    "\n",
    "    def addAnalyzer(self, analyzer: _AnalyzerObject):\n",
    "        return IncreamentalAnalysis(self.analyzer + [analyzer])\n",
    "\n",
    "    def generate(self, spark: SparkSession):\n",
    "        return list(\n",
    "            map(lambda analyzer: analyzer._set_jvm(spark._jvm)._analyzer_jvm, self.analyzer)\n",
    "        )\n",
    "\n",
    "\n",
    "class IcreamentalRunner:\n",
    "    def __init__(self, spark: SparkSession, analyzers: List):\n",
    "        self.spark = spark\n",
    "        self._jsparkSession = spark._jsparkSession\n",
    "        self._jvm = self.spark._jvm\n",
    "        if isinstance(analyzers,List):\n",
    "            self.analysis = self._jvm.com.amazon.deequ.analyzers.Analysis(\n",
    "                to_scala_seq(self._jvm, analyzers)\n",
    "            )\n",
    "        else:\n",
    "            self.analysis = analyzers\n",
    "        self.analysisRunner = (\n",
    "            spark._jvm.com.amazon.deequ.analyzers.runners.AnalysisRunner\n",
    "        )\n",
    "\n",
    "    def run(self,df, state, firstTime: bool = True):\n",
    "        #state = self._jvm.com.amazon.deequ.analyzers.HdfsStateProvider(\n",
    "        #    self._jsparkSession, statePath, 10, False\n",
    "        #)\n",
    "        storageLevelOfGroupedDataForMultiplePasses = getattr(\n",
    "            self._jvm.com.amazon.deequ.analyzers.runners.AnalysisRunner, \"run$default$5\"\n",
    "        )()\n",
    "        if firstTime:\n",
    "            print(\"First Run\")\n",
    "            saveStatesWith = self._jvm.scala.Some(state)\n",
    "            aggregateWith = getattr(\n",
    "                self._jvm.com.amazon.deequ.analyzers.runners.AnalysisRunner,\n",
    "                \"run$default$3\",\n",
    "            )()\n",
    "        else:\n",
    "            print(\"Incremental Run\")\n",
    "            aggregateWith = self._jvm.scala.Some(state)\n",
    "            saveStatesWith = getattr(\n",
    "                self._jvm.com.amazon.deequ.analyzers.runners.AnalysisRunner,\n",
    "                \"run$default$4\",\n",
    "            )()\n",
    "        print(saveStatesWith,aggregateWith)\n",
    "        return self.analysisRunner.run(\n",
    "            df._jdf,\n",
    "            self.analysis,\n",
    "            aggregateWith,\n",
    "            saveStatesWith,\n",
    "            storageLevelOfGroupedDataForMultiplePasses,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c45b896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON']='python'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder.config(\"spark.jars\",\"deequ-2.0.1-spark-3.2.jar\").getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45f18ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "day1 = spark.createDataFrame([\n",
    "  (1, \"Thingy A\", \"awesome thing.\", \"high\", 0),\n",
    "  (2, \"Thingy B\", None, \"low\", 0),\n",
    "  (3, None, None, None, 5)]).toDF(*['id','name','description','demand','quantity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a924020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydeequ.analyzers import *\n",
    "\n",
    "analysis = (IncreamentalAnalysis()\n",
    " .addAnalyzer(Size())\n",
    " .addAnalyzer(Completeness(\"id\"))\n",
    " .addAnalyzer(Completeness(\"name\"))\n",
    " .addAnalyzer(Completeness(\"description\"))\n",
    " .generate(spark)\n",
    ")\n",
    "\n",
    "analysisRunner = IcreamentalRunner(spark,analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "481ef032",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = spark._jvm.com.amazon.deequ.analyzers.InMemoryStateProvider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25f7472f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Run\n"
     ]
    }
   ],
   "source": [
    "data = analysisRunner.run(day1,state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3611b9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------------+\n",
      "|   instance|        name|             value|\n",
      "+-----------+------------+------------------+\n",
      "|          *|        Size|               3.0|\n",
      "|         id|Completeness|               1.0|\n",
      "|       name|Completeness|0.6666666666666666|\n",
      "|description|Completeness|0.3333333333333333|\n",
      "+-----------+------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gopal\\miniconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    }
   ],
   "source": [
    "AnalyzerContext.successMetricsAsDataFrame(spark, data).drop('entity').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44444e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "day2 = spark.createDataFrame([\n",
    "  (4, \"Thingy D\", None, \"low\", 10),\n",
    "  (5, \"Thingy E\", 'awesome', \"high\", 12)\n",
    "]).toDF(*['id','name','description','demand','quantity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fc610624",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = spark._jvm.com.amazon.deequ.analyzers.InMemoryStateProvider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3c3c862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incremental Run\n"
     ]
    }
   ],
   "source": [
    "data = analysisRunner.run(day2,state,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67272d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----+\n",
      "|   instance|        name|value|\n",
      "+-----------+------------+-----+\n",
      "|          *|        Size|  5.0|\n",
      "|         id|Completeness|  1.0|\n",
      "|       name|Completeness|  0.8|\n",
      "|description|Completeness|  0.4|\n",
      "+-----------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AnalyzerContext.successMetricsAsDataFrame(spark, data).drop('entity').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "500209b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "deDF = spark.createDataFrame([\n",
    "  (1, \"ManufacturerA\", \"DE\"),\n",
    "  (2, \"ManufacturerB\", \"DE\")]).toDF(*['id','name','country'])\n",
    "\n",
    "usDF = spark.createDataFrame([\n",
    "  (3, \"ManufacturerD\", \"US\"),\n",
    "  (4, \"ManufacturerE\", \"US\"),\n",
    "  (5, \"ManufacturerF\", \"US\")]).toDF(*['id','name','country'])\n",
    "\n",
    "cnDF = spark.createDataFrame([\n",
    "  (6, \"ManufacturerG\", \"CN\"),\n",
    "  (7, \"ManufacturerH\", \"CN\")]).toDF(*['id','name','country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e79f7bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydeequ.checks import *\n",
    "\n",
    "check = (Check(spark, CheckLevel.Warning, \"Review Check\")\n",
    "        .isComplete(\"name\")\n",
    "        .containsURL(\"name\", lambda x : x == 0)\n",
    "        .isContainedIn(\"country\", [\"DE\", \"US\", \"CN\"])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "af47e7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis =  spark._jvm.com.amazon.deequ.analyzers.Analysis(check._Check.requiredAnalyzers().toSeq())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1564e88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis(Vector(Completeness(name,None), PatternMatch(name,(https?|ftp)://[^\\s/$.?#].[^\\s]*,None), Compliance(country contained in DE,US,CN,`country` IS NULL OR `country` IN ('DE','US','CN'),None)))\n"
     ]
    }
   ],
   "source": [
    "print(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c4fdc0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysisRunner = IcreamentalRunner(spark,analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e717f0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "deState = spark._jvm.com.amazon.deequ.analyzers.InMemoryStateProvider()\n",
    "usState = spark._jvm.com.amazon.deequ.analyzers.InMemoryStateProvider()\n",
    "cnState = spark._jvm.com.amazon.deequ.analyzers.InMemoryStateProvider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7bc97c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Run\n",
      "Some() None\n"
     ]
    }
   ],
   "source": [
    "deData = analysisRunner.run(deDF,deState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1d91c4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----+\n",
      "|            instance|        name|value|\n",
      "+--------------------+------------+-----+\n",
      "|                name|Completeness|  1.0|\n",
      "|                name|PatternMatch|  0.0|\n",
      "|country contained...|  Compliance|  1.0|\n",
      "+--------------------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydeequ.analyzers import *\n",
    "\n",
    "AnalyzerContext.successMetricsAsDataFrame(spark, deData).drop('entity').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "82db7822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Run\n",
      "Some() None\n",
      "+--------------------+------------+-----+\n",
      "|            instance|        name|value|\n",
      "+--------------------+------------+-----+\n",
      "|                name|Completeness|  1.0|\n",
      "|                name|PatternMatch|  0.0|\n",
      "|country contained...|  Compliance|  1.0|\n",
      "+--------------------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usData = analysisRunner.run(usDF,usState)\n",
    "\n",
    "AnalyzerContext.successMetricsAsDataFrame(spark, usData).drop('entity').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "866f9eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Run\n",
      "Some() None\n",
      "+--------------------+------------+-----+\n",
      "|            instance|        name|value|\n",
      "+--------------------+------------+-----+\n",
      "|                name|Completeness|  1.0|\n",
      "|                name|PatternMatch|  0.0|\n",
      "|country contained...|  Compliance|  1.0|\n",
      "+--------------------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnData = analysisRunner.run(cnDF,cnState)\n",
    "\n",
    "AnalyzerContext.successMetricsAsDataFrame(spark, cnData).drop('entity').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "582fbffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_analysisRunner = spark._jvm.com.amazon.deequ.analyzers.runners.AnalysisRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "189bb0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveStatesWith = getattr(\n",
    "                spark._jvm.com.amazon.deequ.analyzers.runners.AnalysisRunner,\n",
    "                \"runOnAggregatedStates$default$4\",\n",
    "            )()\n",
    "storageLevelOfGroupedDataForMultiplePasses = getattr(\n",
    "                spark._jvm.com.amazon.deequ.analyzers.runners.AnalysisRunner,\n",
    "                \"runOnAggregatedStates$default$5\",\n",
    "            )()\n",
    "metricsRepository = getattr(\n",
    "                spark._jvm.com.amazon.deequ.analyzers.runners.AnalysisRunner,\n",
    "                \"runOnAggregatedStates$default$6\",\n",
    "            )()\n",
    "saveOrAppendResultsWithKey = getattr(\n",
    "                spark._jvm.com.amazon.deequ.analyzers.runners.AnalysisRunner,\n",
    "                \"runOnAggregatedStates$default$7\",\n",
    "            )()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7ead4c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydeequ.scala_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5c6defcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggData = java_analysisRunner.runOnAggregatedStates(\n",
    "                    usDF._jdf.schema(),\n",
    "                    analysis,\n",
    "                    to_scala_seq(spark._jvm, [deState,usState,cnState]),\n",
    "                    saveStatesWith,\n",
    "                    storageLevelOfGroupedDataForMultiplePasses,\n",
    "                    metricsRepository,\n",
    "                    saveOrAppendResultsWithKey\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f351e6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----+\n",
      "|            instance|        name|value|\n",
      "+--------------------+------------+-----+\n",
      "|                name|Completeness|  1.0|\n",
      "|                name|PatternMatch|  0.0|\n",
      "|country contained...|  Compliance|  1.0|\n",
      "+--------------------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AnalyzerContext.successMetricsAsDataFrame(spark, aggData).drop('entity').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ce7c5d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "updatedUS = spark.createDataFrame([\n",
    "  (3, \"ManufacturerDNew\", \"US\"),\n",
    "  (4, None, \"US\"),\n",
    "  (5, \"ManufacturerFNew http://clickme.com\", \"US\")]).toDF(*['id','name','country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "180bbcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "updatedUsState = spark._jvm.com.amazon.deequ.analyzers.InMemoryStateProvider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bff6302c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Run\n",
      "Some(Completeness(name,None) => NumMatchesAndCount(2,3)\n",
      "PatternMatch(name,(https?|ftp)://[^\\s/$.?#].[^\\s]*,None) => NumMatchesAndCount(1,3)\n",
      "Compliance(country contained in DE,US,CN,`country` IS NULL OR `country` IN ('DE','US','CN'),None) => NumMatchesAndCount(3,3)\n",
      ") None\n",
      "+--------------------+------------+------------------+\n",
      "|            instance|        name|             value|\n",
      "+--------------------+------------+------------------+\n",
      "|                name|Completeness|0.6666666666666666|\n",
      "|                name|PatternMatch|0.3333333333333333|\n",
      "|country contained...|  Compliance|               1.0|\n",
      "+--------------------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updatedUsData = analysisRunner.run(updatedUS,updatedUsState)\n",
    "\n",
    "AnalyzerContext.successMetricsAsDataFrame(spark, updatedUsData).drop('entity').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dc3543ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggData = java_analysisRunner.runOnAggregatedStates(\n",
    "                    usDF._jdf.schema(),\n",
    "                    analysis,\n",
    "                    to_scala_seq(spark._jvm, [deState,updatedUsState,cnState]),\n",
    "                    saveStatesWith,\n",
    "                    storageLevelOfGroupedDataForMultiplePasses,\n",
    "                    metricsRepository,\n",
    "                    saveOrAppendResultsWithKey\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cb6cf104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-------------------+\n",
      "|            instance|        name|              value|\n",
      "+--------------------+------------+-------------------+\n",
      "|                name|Completeness| 0.8571428571428571|\n",
      "|                name|PatternMatch|0.14285714285714285|\n",
      "|country contained...|  Compliance|                1.0|\n",
      "+--------------------+------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AnalyzerContext.successMetricsAsDataFrame(spark, aggData).drop('entity').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3215ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
